{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anastasiiatodoshchuk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [1] Data Cleaning and creating DataFrame with sentences using Tokenizer (function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def data_rewrite(tsv_df):\n",
    "\n",
    "    words = tsv_df .CHAPTER[1:].values\n",
    "    text=' '.join(words)\n",
    "\n",
    "    # text cleaning\n",
    "    text = re.sub(r'\\s([?.!,:;\"](?:\\s|$))', r'\\1', text)\n",
    "    text = re.sub(r'\\s([’](?:\\s|$))', r'\\1', text)\n",
    "    text = re.sub(r'([‘])\\s', r'\\1', text)\n",
    "    text = re.sub('\\s([‘])', ' \"', text)\n",
    "    text = re.sub('([’])\\s', '\" ', text)\n",
    "    text = re.sub('([’])[,]', '\",', text)\n",
    "    text = re.sub('([’])[;]', '\";', text)\n",
    "\n",
    "    # adding annotated event\n",
    "    data_annotated = pd.DataFrame(sent_tokenize(text), columns=['sent_token'])\n",
    "    data_annotated = get_events_annotated(tsv_df, data_annotated)\n",
    "\n",
    "    # removing direct speech\n",
    "    text_speech = re.sub(\"\\\".*?[.,!?;\\\\-]\\\"\", ' \"SPEECH.\"',  text)\n",
    "\n",
    "    # creating DataFrame with sentences using tokenizer\n",
    "    data_speechless = pd.DataFrame(sent_tokenize(text_speech), columns = ['sent_token'])\n",
    "\n",
    "    return data_annotated, data_speechless\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [1.1] Getting annotated events from tsv dataframe (sub-function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_events_annotated(tsv_df, data):\n",
    "\n",
    "    ind = tsv_df[tsv_df.O=='EVENT'].index\n",
    "\n",
    "    events = []\n",
    "    for i in range(len(ind)):\n",
    "        events.append(list(tsv_df.loc[ind[i]-1:ind[i]+2, 'CHAPTER'].values))\n",
    "    e = [\" \".join([word for word in a]) for a in events]\n",
    "    data['annotated_events'] = None\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        sentence = data['sent_token'][i]\n",
    "        list_of_events=[]\n",
    "        for event in e:\n",
    "            if event in sentence:\n",
    "                list_of_events.append(event.split()[1])\n",
    "        data.loc[i, 'annotated_events'] = str(list_of_events)\n",
    "\n",
    "    data['annotated_events'] = data['annotated_events'].apply(ast.literal_eval)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [2] Splitting complex sentences into simple sub-sentences (function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_sub_sentences(data):\n",
    "\n",
    "    data['sub_sentences'] = None\n",
    "    previous_token = ''\n",
    "    sub_sentences = []\n",
    "    sub_sentence = ''\n",
    "\n",
    "\n",
    "    for row in range(data.shape[0]):\n",
    "\n",
    "        doc_sent = nlp(data.sent_token[row])\n",
    "        for token in doc_sent:\n",
    "\n",
    "            # token is a part of the next sentence\n",
    "            # CCONJ is 'and', 'but'\n",
    "            if token.pos_ == 'CCONJ' and previous_token:\n",
    "\n",
    "                # exammple: ..., and ...\"\n",
    "                if previous_token.pos_ == 'PUNCT':\n",
    "                    sub_sentences.append(sub_sentence)\n",
    "                    sub_sentence = ''\n",
    "                    sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "            # SCONJ is 'because', 'that', 'so'\n",
    "            elif token.pos_ == 'SCONJ' and previous_token:\n",
    "\n",
    "                # token is a part of the current sentence\n",
    "                if previous_token.pos_ == 'SCONJ':\n",
    "                    sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "                # token is a part of the current sentence\n",
    "                if previous_token.pos_ == 'CCONJ':\n",
    "                    sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "                # token is a part of the current sentence (Row 3: \"ones upon a time...\" case)\n",
    "                if len(sub_sentence.split())==1:\n",
    "                    sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "                # token is a part of the new sentence\n",
    "                # example> Row 4: long ago THAT Pippi did n't remember her at all\n",
    "                else:\n",
    "                    sub_sentences.append(sub_sentence)\n",
    "                    sub_sentence = ''\n",
    "                    sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "            # token is a part of the current sentence\n",
    "            elif token.text == ';':\n",
    "                #sub_sentence = sub_sentence + token.text\n",
    "                sub_sentence = sub_sentence + '.'\n",
    "                sub_sentences.append(sub_sentence)\n",
    "                sub_sentence = ''\n",
    "\n",
    "            else:\n",
    "                sub_sentence = sub_sentence + token.text + ' '\n",
    "\n",
    "            previous_token = token\n",
    "\n",
    "        sub_sentences.append(sub_sentence)\n",
    "        data.loc[row, 'sub_sentences'] = str(sub_sentences)\n",
    "        previous_token = ''\n",
    "        sub_sentences = []\n",
    "        sub_sentence = ''\n",
    "\n",
    "\n",
    "    data['sub_sentences'] = data['sub_sentences'].apply(ast.literal_eval)\n",
    "\n",
    "    data['count_sub'] = None\n",
    "    for row in range(data.shape[0]):\n",
    "        data.loc[row, 'count_sub'] = len(data.sub_sentences[row])\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [3] Applying Q1 (agent), Q2 (action), and Q3(patient) to annotated dataset (function)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# This function is looking for agents, events, and patients on the level of sub-sentences.\n",
    "def get_agents_events_patients(data):\n",
    "    agent_roles = ['nsubj', 'nsubjpass']\n",
    "    verb_roles = ['ROOT', 'conj', 'ccomp', 'advcl', 'relcl']\n",
    "    patient_roles = ['dobj', 'attr']\n",
    "\n",
    "    annex_agent_roles = ['conj', 'compound', 'advcl']\n",
    "\n",
    "    data['agents_in_sent'] = None\n",
    "    data['events_in_sent'] = None\n",
    "    data['patients_in_sent'] = None\n",
    "\n",
    "    agents_in_sent = []\n",
    "    events_in_sent = []\n",
    "    patients_in_sent = []\n",
    "    event = ''\n",
    "    agent = ''\n",
    "    patient = None\n",
    "\n",
    "    final_agents = []\n",
    "    final_events = []\n",
    "    final_patients = []\n",
    "\n",
    "    for row in range(data.shape[0]):\n",
    "        for subsent in data['sub_sentences'][row]:\n",
    "            doc = nlp(subsent)\n",
    "            for word in doc:\n",
    "\n",
    "                if (word.dep_ in agent_roles) and (word.head.dep_ in verb_roles):\n",
    "\n",
    "                    agent = word.text\n",
    "                    event = word.head.text\n",
    "\n",
    "                    agents_in_sent.append(agent)\n",
    "                    events_in_sent.append(event)\n",
    "\n",
    "                    # -- are there a PATIENT for the found AGENT and EVENT pair? --\n",
    "                    for echild in word.head.children:\n",
    "                        # -- if there's a patient and if it's the 1st one - add it\n",
    "                        # -- (it goes in set with main agent and event).\n",
    "                        if echild.dep_ in patient_roles:\n",
    "                            patient = echild.text\n",
    "                            patients_in_sent.append(patient)\n",
    "\n",
    "                    # -- if no patient was found, than add the None. If it was found, make it None\n",
    "                    if patient:\n",
    "                        patient = None\n",
    "                    else:\n",
    "                        patients_in_sent.append(patient)\n",
    "\n",
    "                    # -- different agent-children with the same parent event --\n",
    "                    for achild in word.children:\n",
    "                        if achild.dep_ in annex_agent_roles:\n",
    "                            agent = achild.text\n",
    "                            agents_in_sent.append(agent)\n",
    "                            events_in_sent.append(event)\n",
    "\n",
    "                    # -- different event-children with the same parent agent --\n",
    "                    agent = word.text\n",
    "                    for echild in word.head.children:\n",
    "                        if echild.dep_ in annex_agent_roles:\n",
    "                            event = echild.text\n",
    "                            agents_in_sent.append(agent)\n",
    "                            events_in_sent.append(event)\n",
    "\n",
    "                    # -- combinations agent-children with event-children --\n",
    "                    for achild in word.children:\n",
    "                        if achild.dep_ in annex_agent_roles:\n",
    "                            agent = achild.text\n",
    "                            for echild in word.head.children:\n",
    "                                if echild.dep_ in annex_agent_roles:\n",
    "                                    event = echild.text\n",
    "                                    agents_in_sent.append(agent)\n",
    "                                    events_in_sent.append(event)\n",
    "\n",
    "\n",
    "            final_agents.append(agents_in_sent)\n",
    "            final_events.append(events_in_sent)\n",
    "            final_patients.append(patients_in_sent)\n",
    "\n",
    "            agents_in_sent = []\n",
    "            events_in_sent = []\n",
    "            patients_in_sent = []\n",
    "\n",
    "        data['agents_in_sent'][row] = final_agents\n",
    "        data['events_in_sent'][row] = final_events\n",
    "        data['patients_in_sent'][row] = final_patients\n",
    "\n",
    "        final_agents = []\n",
    "        final_events = []\n",
    "        final_patients = []\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [4] Evaluation of Q2 action (comparison with the annotated actions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def evaluation(evaluation_df):\n",
    "\n",
    "    evaluation_df['TruePositive'] = 0\n",
    "    evaluation_df['FalseNegative'] = 0\n",
    "    evaluation_df['FalsePositive'] = 0\n",
    "\n",
    "    for i in range(evaluation_df.shape[0]):\n",
    "        evaluation_df['events_in_sent'][i] = [item for sublist in evaluation_df['events_in_sent'][i] for item in sublist]\n",
    "        evaluation_df['events_in_sent'][i] = list(dict.fromkeys(evaluation_df['events_in_sent'][i]))\n",
    "    for i in range(evaluation_df.shape[0]):\n",
    "        if evaluation_df['annotated_events'][i]:\n",
    "            for true_event in evaluation_df['annotated_events'][i]:\n",
    "                if true_event in evaluation_df['events_in_sent'][i]:\n",
    "                    evaluation_df['TruePositive'][i]+=1\n",
    "                else:\n",
    "                    evaluation_df['FalseNegative'][i]+=1\n",
    "        if evaluation_df['events_in_sent'][i]:\n",
    "            for found_event in evaluation_df['events_in_sent'][i]:\n",
    "                if found_event not in evaluation_df['annotated_events'][i]:\n",
    "                    evaluation_df['FalsePositive'][i]+=1\n",
    "\n",
    "    Precision = 0\n",
    "    Recall = 0\n",
    "    F1_score = 0\n",
    "    TP_score = 0\n",
    "    FP_score = 0\n",
    "    FN_score = 0\n",
    "\n",
    "    for i in range(evaluation_df.shape[0]):\n",
    "        if evaluation_df['annotated_events'][i] and evaluation_df['events_in_sent'][i]:\n",
    "\n",
    "            TP_score+= evaluation_df['TruePositive'][i]\n",
    "            FP_score+= evaluation_df['FalsePositive'][i]\n",
    "            FN_score+= evaluation_df['FalseNegative'][i]\n",
    "\n",
    "    # we set F1_score, Precision, and Recall as nones in case they can't be calculated\n",
    "    # (the case of only one literary story - \"The Magnificent Ambersons\")\n",
    "    if TP_score == 0 and FP_score == 0 or TP_score == 0 and FN_score == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    Precision = TP_score/ (TP_score + FP_score)\n",
    "    Recall = TP_score / (TP_score + FN_score)\n",
    "    F1_score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "    return round(F1_score, 4), round(Precision, 4), round(Recall, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Additional functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def add_results(df, name, F_score, Precision, Recall):\n",
    "    df = df.append({'annotated_file': name, 'F_score': F_score, 'Precision': Precision, 'Recall': Recall}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def final_result(df, metric):\n",
    "    sum = df[metric].sum()\n",
    "    num = df[metric].notnull().sum()\n",
    "\n",
    "    return round(sum/num, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Results] Looping through annotated files and applying functions [1]-[4] + saving results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-b5331c76041d>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_df['events_in_sent'][i] = [item for sublist in evaluation_df['events_in_sent'][i] for item in sublist]\n",
      "<ipython-input-20-b5331c76041d>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_df['events_in_sent'][i] = list(dict.fromkeys(evaluation_df['events_in_sent'][i]))\n",
      "<ipython-input-20-b5331c76041d>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_df['TruePositive'][i]+=1\n",
      "<ipython-input-20-b5331c76041d>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_df['FalsePositive'][i]+=1\n",
      "<ipython-input-20-b5331c76041d>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_df['FalseNegative'][i]+=1\n"
     ]
    }
   ],
   "source": [
    "directory = 'litbank-master/events/tsv/'\n",
    "res_df = pd.DataFrame(columns=['annotated_file', 'F_score', 'Precision', 'Recall'])\n",
    "F_score = None\n",
    "Precision = None\n",
    "Recall = None\n",
    "# loop through all the annotated files\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        tsv_df = pd.read_csv(path, sep='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        tsv_df.columns = ['CHAPTER', 'O']\n",
    "\n",
    "        data_annotated, data_speechless = data_rewrite(tsv_df)\n",
    "        if data_annotated.shape[0] == data_speechless.shape[0] and not data_annotated[data_annotated[\"annotated_events\"].astype(bool)].empty:\n",
    "            data_speechless['annotated_events'] = data_annotated['annotated_events']\n",
    "\n",
    "        data = get_sub_sentences(data_annotated)\n",
    "        data = get_agents_events_patients(data)\n",
    "        if 'annotated_events' in data.columns:\n",
    "            evaluation_df = data.loc[:, ['annotated_events', 'events_in_sent']]\n",
    "            F_score, Precision, Recall = evaluation(evaluation_df)\n",
    "        res_df = add_results(res_df, filename, F_score, Precision, Recall)\n",
    "        F_score = None\n",
    "        Precision = None\n",
    "        Recall = None\n",
    "\n",
    "res_df.to_csv('evaluation_results_LitBank.csv', encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Printing the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE AVERAGE F-SCORE IS 0.4957\n",
      "THE AVERAGE PRECISION IS 0.4235\n",
      "THE AVERAGE RECALL IS 0.613\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv('evaluation_results.csv', index_col=None)\n",
    "print(f'THE AVERAGE F-SCORE IS {final_result(res_df, \"F_score\")}')\n",
    "print(f'THE AVERAGE PRECISION IS {final_result(res_df, \"Precision\")}')\n",
    "print(f'THE AVERAGE RECALL IS {final_result(res_df, \"Recall\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}